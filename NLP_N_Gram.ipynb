{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data preprocessing, the text from all_text is read, if the file doesnpt esixt the data is processed from txt folder and merge all text and save to all_text.txt for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLeoafBEqzuj",
    "outputId": "22d05b7a-e057-4c6d-b6e9-98b560202189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training Text\n",
      "CHAPTER I. START IN LIFE\n",
      "\n",
      "\n",
      "I was born in the year 1632, in the city of York, of a good family,\n",
      "thoug\n",
      "====== Validate Text\n",
      "Chapter I. Into the Primitive\n",
      "\n",
      "\n",
      " Old longings nomadic leap,\n",
      "Chafing at custom s chain;\n",
      "Again from it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Training text\n",
    "train_file = 'EN_train.txt'\n",
    "train_text = ''\n",
    "with open(train_file, 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "    train_text = file_content\n",
    "\n",
    "print(\"====== Training Text\")\n",
    "print(train_text[:100])\n",
    "\n",
    "# Vallidation text\n",
    "validate_file = 'EN_validate.txt'\n",
    "validate_text = ''\n",
    "with open(validate_file, 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "    validate_text = file_content\n",
    "    \n",
    "\n",
    "print(\"====== Validate Text\")\n",
    "print(validate_text[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Defining N gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "        self.vocab = set()  # Vocabulary of all unique words (for smoothing)\n",
    "        \n",
    "        # Check if M1 GPU (Metal) or CUDA is available, else fall back to CPU\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")  # for mac with M series SoC\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")  # CUDA GPU\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")  # Default to CPU\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple whitespace-based tokenization.\"\"\"\n",
    "        return text.split()\n",
    "\n",
    "    def ngrams(self, corpus):\n",
    "        \"\"\"\n",
    "        Generate n-grams from the corpus with padding.\n",
    "        \"\"\"\n",
    "        # Pad the corpus with <s> and </s> tokens\n",
    "        padded_corpus = ['<s>'] * (self.n - 1) + corpus + ['</s>']\n",
    "        return [tuple(padded_corpus[i:i + self.n]) for i in range(len(padded_corpus) - self.n + 1)]\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the n-gram model on the provided corpus.\n",
    "        Args:\n",
    "            corpus (list): List of tokenized words.\n",
    "        \"\"\"\n",
    "        # Tokenize the corpus and create n-grams\n",
    "        ngrams_list = self.ngrams(corpus)\n",
    "        \n",
    "        # Add each word to the vocabulary set\n",
    "        self.vocab.update(corpus)\n",
    "        \n",
    "        # Count n-grams and contexts\n",
    "        for ngram in ngrams_list:\n",
    "            context = ngram[:-1]  # First (n-1) words\n",
    "            token = ngram[-1]     # Last word\n",
    "            self.ngram_counts[context][token] += 1\n",
    "            self.context_counts[context] += 1\n",
    "\n",
    "    def predict_next(self, context):\n",
    "        \"\"\"\n",
    "        Predict the next word based on the given context using add-one smoothing.\n",
    "        Args:\n",
    "            context (tuple): Tuple of (n-1) words as the context.\n",
    "        Returns:\n",
    "            dict: Probabilities of next possible words.\n",
    "        \"\"\"\n",
    "        if context not in self.ngram_counts:\n",
    "            return None\n",
    "\n",
    "        # Get the counts of possible next words for the context\n",
    "        possible_next_words = self.ngram_counts[context]\n",
    "        total_count = self.context_counts[context]\n",
    "        \n",
    "        # Apply add-one smoothing: Add 1 to each count and include the vocabulary size in the denominator\n",
    "        smoothed_probabilities = {}\n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Transfer calculations to the selected device (CPU, CUDA, or MPS)\n",
    "        for word in self.vocab:\n",
    "            count = possible_next_words[word] if word in possible_next_words else 0\n",
    "            smoothed_prob = (count + 1) / (total_count + vocab_size)\n",
    "            smoothed_probabilities[word] = smoothed_prob\n",
    "\n",
    "        return smoothed_probabilities\n",
    "\n",
    "    def evaluate_perplexity(self, corpus):\n",
    "        \"\"\"\n",
    "        Calculate the perplexity of the model on a given corpus.\n",
    "        Args:\n",
    "            corpus (list): List of tokenized words.\n",
    "        Returns:\n",
    "            float: Perplexity score.\n",
    "        \"\"\"\n",
    "        ngrams_list = self.ngrams(corpus)\n",
    "        \n",
    "        log_prob_sum = 0\n",
    "        N = len(corpus)  # Total number of words\n",
    "        \n",
    "        for ngram in ngrams_list:\n",
    "            context = ngram[:-1]  # First (n-1) words\n",
    "            token = ngram[-1]     # Last word\n",
    "            \n",
    "            # Get the smoothed probabilities for the context\n",
    "            smoothed_probs = self.predict_next(context)\n",
    "            \n",
    "            if smoothed_probs is None or token not in smoothed_probs:\n",
    "                # If no valid probability exists (e.g., unseen context), assign a small probability\n",
    "                smoothed_prob = 1 / (len(self.vocab) + 1)  # Small probability\n",
    "            else:\n",
    "                # Get the probability of the actual token\n",
    "                smoothed_prob = smoothed_probs[token]\n",
    "            \n",
    "            # Add log of the probability to the log sum\n",
    "            log_prob_sum += math.log(smoothed_prob, 2)  # Log probability (base 2)\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        perplexity = math.pow(2, -log_prob_sum / N)\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Text: 27282132\n",
      "Tokens: ['CHAPTER', 'I', '.', 'START', 'IN', 'LIFE', 'I', 'was', 'born', 'in', 'the', 'year', '1632', ',', 'in', 'the', 'city', 'of', 'York', ',']\n",
      "Tokens: ['CHAPTER', 'I', '.', 'START', 'IN', 'LIFE', 'I', 'was', 'born', 'in', 'the', 'year', '1632', ',', 'in', 'the', 'city', 'of', 'York', ',']\n",
      "Only taking Tokens: 100000\n",
      "====== Validation Token\n",
      "Total Validation Text: 4418100\n",
      "Validation Tokens: ['Chapter', 'I', '.', 'Into', 'the', 'Primitive', 'Old', 'longings', 'nomadic', 'leap', ',', 'Chafing', 'at', 'custom', 's', 'chain', ';', 'Again', 'from', 'its']\n",
      "Validation Tokens: ['Chapter', 'I', '.', 'Into', 'the', 'Primitive', 'Old', 'longings', 'nomadic', 'leap', ',', 'Chafing', 'at', 'custom', 's', 'chain', ';', 'Again', 'from', 'its']\n",
      "Only taking Validation Tokens: 100000\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"Total Text:\", len(train_text))\n",
    "tokens = word_tokenize(train_text, language='english', preserve_line=False)\n",
    "print(\"Tokens:\", tokens[:20])\n",
    "\n",
    "      \n",
    "token_lengths = 100000 \n",
    "tokens = tokens[:token_lengths]\n",
    "print(\"Tokens:\", tokens[:20])\n",
    "\n",
    "print(\"Only taking Tokens:\", len(tokens))\n",
    "\n",
    "print(\"====== Validation Token\")\n",
    "\n",
    "print(\"Total Validation Text:\", len(validate_text))\n",
    "validate_tokens = word_tokenize(validate_text, language='english', preserve_line=False)\n",
    "print(\"Validation Tokens:\", validate_tokens[:20])\n",
    "\n",
    "      \n",
    "token_lengths = 100000 \n",
    "validate_tokens = validate_tokens[:token_lengths]\n",
    "print(\"Validation Tokens:\", validate_tokens[:20])\n",
    "\n",
    "print(\"Only taking Validation Tokens:\", len(validate_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Input: ()\n",
      "Perplexity of the model: 607.0094541846838\n",
      "1st Word: ,, Probability: 0.07696310804545498\n",
      "2nd Word: I, Probability: 0.039342587073149964\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# For digram, n=1\n",
    "model = NGramModel(n=1)\n",
    "model.train(tokens)\n",
    "# Save the trained model to a file\n",
    "with open('unigram_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Sample corpus (tokenized)\n",
    "context = ()\n",
    "print(f\"Input: {context}\")\n",
    "\n",
    "# Evaluate the model's perplexity on the same corpus (you can use a test corpus)\n",
    "perplexity = model.evaluate_perplexity(validate_tokens)\n",
    "print(f\"Perplexity of the model: {perplexity}\")\n",
    "predictions = model.predict_next(context)\n",
    "                                 \n",
    "\n",
    "# Sort the dictionary by probability in descending order and get the top 2 entries\n",
    "top_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "# Extract the top 1st and 2nd word and their probabilities\n",
    "first_word, first_prob = top_predictions[0]\n",
    "second_word, second_prob = top_predictions[1]\n",
    "\n",
    "# Print the results\n",
    "print(f\"1st Word: {first_word}, Probability: {first_prob}\")\n",
    "print(f\"2nd Word: {second_word}, Probability: {second_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Input: ('nursing',)\n",
      "Perplexity of the model: 595.0719326577995\n",
      "1st Word: it, Probability: 0.0003516792685071215\n",
      "2nd Word: slipped, Probability: 0.00017583963425356076\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# For digram, n=2\n",
    "model = NGramModel(n=2)\n",
    "model.train(tokens)\n",
    "# Save the trained model to a file\n",
    "with open('digram_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Sample corpus (tokenized)\n",
    "corpus = tokens\n",
    "context = ('nursing',)\n",
    "\n",
    "print(f\"Input: {context}\")\n",
    "\n",
    "# Evaluate the model's perplexity on the same corpus (you can use a test corpus)\n",
    "perplexity = model.evaluate_perplexity(corpus)\n",
    "print(f\"Perplexity of the model: {perplexity}\")\n",
    "predictions = model.predict_next(context)\n",
    "                                 \n",
    "\n",
    "# Sort the dictionary by probability in descending order and get the top 2 entries\n",
    "top_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "# Extract the top 1st and 2nd word and their probabilities\n",
    "first_word, first_prob = top_predictions[0]\n",
    "second_word, second_prob = top_predictions[1]\n",
    "\n",
    "# Print the results\n",
    "print(f\"1st Word: {first_word}, Probability: {first_prob}\")\n",
    "print(f\"2nd Word: {second_word}, Probability: {second_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Input: ('My', 'father')\n",
      "Perplexity of the model: 1846.0709715429957\n",
      "1st Word: ,, Probability: 0.0005274261603375527\n",
      "2nd Word: slipped, Probability: 0.00017580872011251758\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# For digram, n=3\n",
    "model = NGramModel(n=3)\n",
    "model.train(tokens)\n",
    "# Save the trained model to a file\n",
    "with open('trigram_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Sample corpus (tokenized)\n",
    "corpus = tokens\n",
    "context = ('My', 'father', )\n",
    "\n",
    "print(f\"Input: {context}\")\n",
    "# Evaluate the model's perplexity on the same corpus (you can use a test corpus)\n",
    "perplexity = model.evaluate_perplexity(corpus)\n",
    "print(f\"Perplexity of the model: {perplexity}\")\n",
    "predictions = model.predict_next(context)\n",
    "\n",
    "\n",
    "# Sort the dictionary by probability in descending order and get the top 2 entries\n",
    "top_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "# Extract the top 1st and 2nd word and their probabilities\n",
    "first_word, first_prob = top_predictions[0]\n",
    "second_word, second_prob = top_predictions[1]\n",
    "\n",
    "# Print the results\n",
    "print(f\"1st Word: {first_word}, Probability: {first_prob}\")\n",
    "print(f\"2nd Word: {second_word}, Probability: {second_prob}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
